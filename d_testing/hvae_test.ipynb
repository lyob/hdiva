{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3a1128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/ceph/users/blyo1/projects', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python310.zip', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/lib-dynload', '', '/mnt/home/blyo1/venvs/py310/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(sys.path)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87df2b8",
   "metadata": {},
   "source": [
    "# hVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13919540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from b_models.vae import VariableConvEncoder, VariableConvDecoder, VAE, UNetDecoder\n",
    "\n",
    "num_channels = 1\n",
    "image_dims = 32\n",
    "latent_dims = 5\n",
    "\n",
    "enc = VariableConvEncoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims, \n",
    "    channels=[32, 32, 32], \n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "dec = VariableConvDecoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims,\n",
    "    channels=[32, 32, 32],  # output_channels, kernel_size, stride, padding\n",
    "    output_channels=1,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "vae = VAE(\n",
    "    encoder=enc,\n",
    "    decoder=dec,\n",
    "    kl_reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df269d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "16\n",
      "8\n",
      "torch.Size([4, 1, 3, 3])\n",
      "torch.Size([8, 4, 3, 3])\n",
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "from b_models.vae import VariableConvEncoder\n",
    "\n",
    "enc = VariableConvEncoder(num_channels=1, image_dims=32, latent_dims=5, channels=[4, 8, 16])\n",
    "print(enc.d1_input_dims)\n",
    "print(enc.d2_input_dims)\n",
    "print(enc.d3_input_dims)\n",
    "\n",
    "print(enc.conv_d1.weight.shape)\n",
    "print(enc.conv_d2.weight.shape)\n",
    "print(enc.conv_d3.weight.shape)\n",
    "\n",
    "print(enc.linear_mu.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "647a4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 3])\n",
      "torch.Size([8, 4, 3, 3])\n",
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import EncoderConvBlock\n",
    "\n",
    "example_input = torch.randn(1, 1, 32, 32)\n",
    "enc_block = EncoderConvBlock(num_channels=1, img_dim=example_input.shape[3], latent_dim=5, channels=[4, 8, 16])\n",
    "\n",
    "for i in [0, 2, 4]:\n",
    "    print(enc_block.conv_network[i].weight.shape)\n",
    "print(enc_block.linear_mu.weight.shape)\n",
    "d, mu, var = enc_block(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "614658c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(output_dim=16, latent_dim=5, channels=[16, 8, 4])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "# for i in range(len(dec_block.deconv_network)):\n",
    "#     if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "#         print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "741b2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([4, 8, 4, 4])\n",
      "torch.Size([8, 16, 4, 4])\n",
      "torch.Size([16, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(latent_in_dim=5, latent_out_dim=2, output_dim=8, channels=[16, 8, 4])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out, _, _ = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "for i in range(len(dec_block.deconv_network)):\n",
    "    if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "        print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f17167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_kernel",
   "language": "python",
   "name": "py310_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
