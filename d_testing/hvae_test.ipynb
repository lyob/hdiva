{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3a1128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/ben/Documents/work/research projects/code/hdiva', '/Users/ben/.pyenv/versions/3.10.18/lib/python310.zip', '/Users/ben/.pyenv/versions/3.10.18/lib/python3.10', '/Users/ben/.pyenv/versions/3.10.18/lib/python3.10/lib-dynload', '', '/Users/ben/.pyenv/versions/pyenv-310/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(sys.path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87df2b8",
   "metadata": {},
   "source": [
    "# hVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effc3b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LadderVAE(\n",
      "  (encoder): ModuleList(\n",
      "    (0): BottomUpBlock(\n",
      "      (conv_block): ConvBlock(\n",
      "        (pre_conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (block): Sequential(\n",
      "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (compress_block): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "        (2): Linear(in_features=32, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottomUpBlock(\n",
      "      (conv_block): ConvBlock(\n",
      "        (pre_conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (block): Sequential(\n",
      "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (compress_block): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "        (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): TopDownBlock(\n",
      "      (expand_block): ConvTranspose2d(3, 32, kernel_size=(8, 8), stride=(1, 1))\n",
      "      (conv_block): ConvBlock(\n",
      "        (pre_conv): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (block): Sequential(\n",
      "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (compress_block): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "        (2): Linear(in_features=32, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TopDownBlock(\n",
      "      (expand_block): ConvTranspose2d(2, 32, kernel_size=(16, 16), stride=(1, 1))\n",
      "      (conv_block): ConvBlock(\n",
      "        (pre_conv): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (block): Sequential(\n",
      "          (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (merge_block): MergeBlock()\n",
      ")\n",
      "Number of trainable parameters: 98055\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import ConvBlock, TopDownBlock, BottomUpBlock, LadderVAE\n",
    "\n",
    "lvae = LadderVAE(input_dim=32, z_dims = [2, 3], channels = [1, 32, 32], num_blocks=2)\n",
    "\n",
    "# c_in, c_out has to match each other, like this: c_in = [a, b, c], c_out = [b, c, d]\n",
    "# which means we can just use one array that lists the channels and iterate through that.\n",
    "\n",
    "print(lvae)\n",
    "\n",
    "# count number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters:\", count_parameters(lvae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e3d863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3052, -0.3198,  0.0447,  ..., -0.2419, -0.2419, -0.2419],\n",
       "          [-0.2451, -0.3446, -0.7794,  ..., -0.2419, -0.2419, -0.2419],\n",
       "          [-0.1077,  0.2963,  0.2705,  ..., -0.1140,  0.1528, -0.0529],\n",
       "          ...,\n",
       "          [ 0.1761,  0.3311,  0.1707,  ..., -0.2296, -0.3409, -0.2611],\n",
       "          [-0.2267, -0.2748, -0.3318,  ..., -0.2259, -0.2309, -0.2465],\n",
       "          [-0.2696, -0.1088, -0.0169,  ..., -0.2786, -0.2448, -0.2487]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "lvae(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6d207",
   "metadata": {},
   "source": [
    "the problem was that I wasn't using the right spatial dimensions in the TopDown blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b69914",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train neural network on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9249c",
   "metadata": {},
   "source": [
    "# scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13919540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from b_models.vae import VariableConvEncoder, VariableConvDecoder, VAE, UNetDecoder\n",
    "\n",
    "num_channels = 1\n",
    "image_dims = 32\n",
    "latent_dims = 5\n",
    "\n",
    "enc = VariableConvEncoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims, \n",
    "    channels=[32, 32, 32], \n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "dec = VariableConvDecoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims,\n",
    "    channels=[32, 32, 32],  # output_channels, kernel_size, stride, padding\n",
    "    output_channels=1,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "vae = VAE(\n",
    "    encoder=enc,\n",
    "    decoder=dec,\n",
    "    kl_reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df269d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "16\n",
      "8\n",
      "torch.Size([4, 1, 3, 3])\n",
      "torch.Size([8, 4, 3, 3])\n",
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "from b_models.vae import VariableConvEncoder\n",
    "\n",
    "enc = VariableConvEncoder(num_channels=1, image_dims=32, latent_dims=5, channels=[4, 8, 16])\n",
    "print(enc.d1_input_dims)\n",
    "print(enc.d2_input_dims)\n",
    "print(enc.d3_input_dims)\n",
    "\n",
    "print(enc.conv_d1.weight.shape)\n",
    "print(enc.conv_d2.weight.shape)\n",
    "print(enc.conv_d3.weight.shape)\n",
    "\n",
    "print(enc.linear_mu.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "647a4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import EncoderConvBlock\n",
    "\n",
    "example_input = torch.randn(1, 1, 32, 32)\n",
    "enc_block = EncoderConvBlock(num_channels=1, img_dim=example_input.shape[3], latent_dim=5, channels=[4, 8, 16])\n",
    "\n",
    "# for i in [0, 2, 4]:\n",
    "#     print(enc_block.conv_network[i].weight.shape)\n",
    "# print(enc_block.linear_mu.weight.shape)\n",
    "d, mu, var = enc_block(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39987035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# testing adaptive avg pool\n",
    "rand_input = torch.randn(1, 16, 7, 7)\n",
    "import torch.nn.functional as F\n",
    "output = F.adaptive_avg_pool2d(rand_input, 1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acea0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import SpatialEncoderConvBlock\n",
    "\n",
    "example_input = torch.randn(1, 1, 32, 32)\n",
    "enc_block = SpatialEncoderConvBlock(num_channels=1, img_dim=example_input.shape[3], latent_dim=5, channels=[4, 8, 16])\n",
    "\n",
    "# for i in [0, 2, 4]:\n",
    "#     print(enc_block.conv_network[i].weight.shape)\n",
    "# print(enc_block.linear_mu.weight.shape)\n",
    "d, mu, var = enc_block(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "614658c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(output_dim=16, latent_dim=5, channels=[16, 8, 4])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "# for i in range(len(dec_block.deconv_network)):\n",
    "#     if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "#         print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "741b2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16, 1, 1])\n",
      "torch.Size([1, 8, 2, 2])\n",
      "torch.Size([1, 8, 2, 2])\n",
      "torch.Size([1, 4, 4, 4])\n",
      "torch.Size([1, 4, 4, 4])\n",
      "torch.Size([1, 16, 8, 8])\n",
      "torch.Size([1, 16, 8, 8])\n",
      "end of deconv network\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(latent_in_dim=5, latent_out_dim=2, output_dim=8, channels=[4, 8, 16])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out, _, _ = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "# for i in range(len(dec_block.deconv_network)):\n",
    "#     if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "#         print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f17167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
