{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3a1128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/ceph/users/blyo1/projects', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python310.zip', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10', '/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/lib-dynload', '', '/mnt/home/blyo1/venvs/py310/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(sys.path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87df2b8",
   "metadata": {},
   "source": [
    "# hVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "effc3b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderVAE(\n",
       "  (encoder): ModuleList(\n",
       "    (0): BottomUpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (pre_conv): Conv2d(1, 5, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (compress_block): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=5, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottomUpBlock(\n",
       "      (conv_block): ConvBlock(\n",
       "        (pre_conv): Conv2d(5, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (compress_block): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=8, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): TopDownBlock(\n",
       "      (expand_block): ConvTranspose2d(3, 8, kernel_size=(16, 16), stride=(1, 1))\n",
       "      (conv_block): ConvBlock(\n",
       "        (pre_conv): ConvTranspose2d(8, 5, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (compress_block): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=5, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TopDownBlock(\n",
       "      (expand_block): ConvTranspose2d(2, 5, kernel_size=(32, 32), stride=(1, 1))\n",
       "      (conv_block): ConvBlock(\n",
       "        (pre_conv): ConvTranspose2d(5, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (block): Sequential(\n",
       "          (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merge_block): MergeBlock()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from b_models.hvae import ConvBlock, TopDownBlock, BottomUpBlock, LadderVAE\n",
    "\n",
    "lvae = LadderVAE(input_dim=32, z_dims = [2, 3], c_in = [1, 5], c_out = [5, 8], num_blocks=1)\n",
    "\n",
    "# c_in, c_out has to match each other, like this: c_in = [a, b, c], c_out = [b, c, d]\n",
    "# which means we can just use one array that lists the channels and iterate through that.\n",
    "\n",
    "lvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37886cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 27615\n"
     ]
    }
   ],
   "source": [
    "# count number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters:\", count_parameters(lvae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7e3d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 8, 16, 16])\n",
      "torch.Size([1, 8, 16, 16])\n",
      "i: 1\n",
      "torch.Size([1, 8, 16, 16])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "j: 0\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 16, 16])\n",
      "torch.Size([1, 8, 16, 16])\n",
      "j: 1\n",
      "torch.Size([1, 8, 16, 16])\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2183, -0.0263, -0.0677,  ...,  0.1675,  0.2517,  0.1360],\n",
       "          [-0.1993,  0.1291, -0.1218,  ...,  0.1240,  0.2605,  0.3418],\n",
       "          [-0.0156, -0.2398,  0.0745,  ...,  0.3267,  0.3944,  0.2639],\n",
       "          ...,\n",
       "          [-0.2797, -0.0138,  0.0043,  ...,  0.1777,  0.1372,  0.2306],\n",
       "          [-0.0066,  0.1069,  0.0167,  ...,  0.3334,  0.2229,  0.3079],\n",
       "          [ 0.0056,  0.0389, -0.0461,  ...,  0.1615,  0.2783,  0.0793]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "lvae(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70852e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv_transpose2d(): argument 'weight' (position 2) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv_transpose2d(): argument 'weight' (position 2) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "F.conv_transpose2d(torch.randn(1, 3), 5, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9249c",
   "metadata": {},
   "source": [
    "# scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13919540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from b_models.vae import VariableConvEncoder, VariableConvDecoder, VAE, UNetDecoder\n",
    "\n",
    "num_channels = 1\n",
    "image_dims = 32\n",
    "latent_dims = 5\n",
    "\n",
    "enc = VariableConvEncoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims, \n",
    "    channels=[32, 32, 32], \n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "dec = VariableConvDecoder(\n",
    "    num_channels,\n",
    "    image_dims,\n",
    "    latent_dims,\n",
    "    channels=[32, 32, 32],  # output_channels, kernel_size, stride, padding\n",
    "    output_channels=1,\n",
    "    bias=True,\n",
    ")\n",
    "\n",
    "vae = VAE(\n",
    "    encoder=enc,\n",
    "    decoder=dec,\n",
    "    kl_reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df269d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "16\n",
      "8\n",
      "torch.Size([4, 1, 3, 3])\n",
      "torch.Size([8, 4, 3, 3])\n",
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "from b_models.vae import VariableConvEncoder\n",
    "\n",
    "enc = VariableConvEncoder(num_channels=1, image_dims=32, latent_dims=5, channels=[4, 8, 16])\n",
    "print(enc.d1_input_dims)\n",
    "print(enc.d2_input_dims)\n",
    "print(enc.d3_input_dims)\n",
    "\n",
    "print(enc.conv_d1.weight.shape)\n",
    "print(enc.conv_d2.weight.shape)\n",
    "print(enc.conv_d3.weight.shape)\n",
    "\n",
    "print(enc.linear_mu.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "647a4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import EncoderConvBlock\n",
    "\n",
    "example_input = torch.randn(1, 1, 32, 32)\n",
    "enc_block = EncoderConvBlock(num_channels=1, img_dim=example_input.shape[3], latent_dim=5, channels=[4, 8, 16])\n",
    "\n",
    "# for i in [0, 2, 4]:\n",
    "#     print(enc_block.conv_network[i].weight.shape)\n",
    "# print(enc_block.linear_mu.weight.shape)\n",
    "d, mu, var = enc_block(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39987035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# testing adaptive avg pool\n",
    "rand_input = torch.randn(1, 16, 7, 7)\n",
    "import torch.nn.functional as F\n",
    "output = F.adaptive_avg_pool2d(rand_input, 1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acea0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 4, 16, 16])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 8, 8, 8])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 16, 4, 4])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import SpatialEncoderConvBlock\n",
    "\n",
    "example_input = torch.randn(1, 1, 32, 32)\n",
    "enc_block = SpatialEncoderConvBlock(num_channels=1, img_dim=example_input.shape[3], latent_dim=5, channels=[4, 8, 16])\n",
    "\n",
    "# for i in [0, 2, 4]:\n",
    "#     print(enc_block.conv_network[i].weight.shape)\n",
    "# print(enc_block.linear_mu.weight.shape)\n",
    "d, mu, var = enc_block(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "614658c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(output_dim=16, latent_dim=5, channels=[16, 8, 4])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "# for i in range(len(dec_block.deconv_network)):\n",
    "#     if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "#         print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "741b2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16, 1, 1])\n",
      "torch.Size([1, 8, 2, 2])\n",
      "torch.Size([1, 8, 2, 2])\n",
      "torch.Size([1, 4, 4, 4])\n",
      "torch.Size([1, 4, 4, 4])\n",
      "torch.Size([1, 16, 8, 8])\n",
      "torch.Size([1, 16, 8, 8])\n",
      "end of deconv network\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "from b_models.hvae import DecoderConvBlock\n",
    "\n",
    "dec_block = DecoderConvBlock(latent_in_dim=5, latent_out_dim=2, output_dim=8, channels=[4, 8, 16])\n",
    "\n",
    "example_input = torch.randn(1, 5)\n",
    "dec_out, _, _ = dec_block(example_input)\n",
    "print(dec_out.shape)\n",
    "\n",
    "# for i in range(len(dec_block.deconv_network)):\n",
    "#     if hasattr(dec_block.deconv_network[i], 'weight'):\n",
    "#         print(dec_block.deconv_network[i].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f17167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_kernel",
   "language": "python",
   "name": "py310_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
